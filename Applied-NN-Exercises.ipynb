{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSugZgHdkajY"
      },
      "source": [
        "# Applied Neural Networks - Exercises\n",
        "\n",
        "**NOTICE:**\n",
        "1. You are allowed to work in groups of up to three people but **have to document** your group's\\\n",
        " members in the top cell of your notebook.\n",
        "2. **Comment your code**, explain what you do (refer to the slides). It will help you understand the topics\\\n",
        " and help me understand your thinking progress. Quality of comments will be graded.\n",
        "3. **Discuss** and analyze your results, **write-down your learnings**. These exercises are no programming\\\n",
        " exercises it is about learning and getting a touch for these methods. Such questions might be asked in the\\\n",
        " final exams.\n",
        " 4. Feel free to **experiment** with these methods. Change parameters think about improvements, write down\\\n",
        " what you learned. This is not only about collecting points for the final grade, it is about understanding\\\n",
        "  the methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFReMDJgkajZ"
      },
      "source": [
        "### Exercise 1 - Data Normalization and Standardization\n",
        "\n",
        "\n",
        "**Summary:** In this exercise you will implement the min-max normalization and standardization and compare it to\\\n",
        "sklearn's implementation. It is important to remember, that we always normalize or standardize for all samples\\\n",
        " over a single feature dimension.\n",
        "\n",
        "\n",
        "**Provided Code:** In the cell below I have provided you with a sample code to initialize some dummy data.\\\n",
        "The parameter ```n_samples``` defines the number of samples we have in the training set (the number of $x_i$)\\\n",
        "while ```n_features``` defines the number of dimensions of each sample feature vector.\n",
        "\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "1. Implement the MinMax Normalization and Standardization.\n",
        "2. Use the ```MinMaxScaler``` and ```StandardScaler``` from sklearn to verify your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "llbzqEfTkajZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "x,y = make_regression(n_samples=10, n_features=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# min aller features\n",
        "x_min = np.min(x, axis=0)\n",
        "\n",
        "# max aller features\n",
        "x_max = np.max(x, axis=0)\n",
        "\n",
        "# min-max scaling fuer alle features\n",
        "x_minmax = (x - x_min) / (x_max - x_min)\n",
        "\n",
        "print(\"Min-Max eigene Implementierung:\\n\", x_minmax)\n",
        "# Man sieht hier sehr gut, dass es fuer jedes Feature ein Wert mit 1 und einen Wert mit 0 gibt (die jeweiligen Min und Max-Werte, koennten auch mehrere sein, wenn min/max oefter vorkommt)"
      ],
      "metadata": {
        "id": "nsmap-tGLGkq",
        "outputId": "e5c104f8-7427-4a11-e389-fb50ab5928c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min-Max eigene Implementierung:\n",
            " [[0.40476159 0.         0.17716244 0.27016344 0.77448957]\n",
            " [0.84425891 0.79578881 0.         0.70276872 0.3206469 ]\n",
            " [0.59055143 0.0340245  0.82246469 0.         0.38462419]\n",
            " [0.22575837 0.74033427 1.         0.61025864 0.77448137]\n",
            " [0.9039843  0.27394465 0.42086525 0.28601121 1.        ]\n",
            " [0.28053095 0.78636198 0.61146714 0.68207098 0.99822039]\n",
            " [0.69682372 0.27853834 0.1201718  0.72880628 0.28458586]\n",
            " [0.         0.19147452 0.55615255 0.6246069  0.87092676]\n",
            " [1.         1.         0.62533191 0.70996065 0.        ]\n",
            " [0.59888451 0.68412728 0.36641176 1.         0.41290151]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mittelwert aller features\n",
        "x_mean = np.mean(x, axis=0)\n",
        "# standardabweichung der features\n",
        "x_std = np.std(x, axis=0)\n",
        "\n",
        "# standardisieren fuer alle features\n",
        "x_standard = (x - x_mean) / x_std\n",
        "\n",
        "print(\"Standardisierung eigene Implementierung:\\n\", x_standard)"
      ],
      "metadata": {
        "id": "W4v39YJeLoQk",
        "outputId": "fd95d8e5-4669-4ee9-f85c-ea5a02d1c9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardisierung eigene Implementierung:\n",
            " [[ 0.11799284 -1.12805277  0.02974996  0.13646306  0.63115847]\n",
            " [-0.92152661 -0.95868573 -0.35009711 -0.69347962  0.54993941]\n",
            " [ 1.15160204  2.55352086 -0.53507049 -0.74207601  0.11676113]\n",
            " [ 1.34497229 -0.18915711  1.52785078  0.39925133 -0.76008397]\n",
            " [-0.98295316  0.49297021  0.39989602  0.91010791 -0.0182514 ]\n",
            " [-1.2115191  -0.28801916 -1.59232798 -0.57159153  0.45440062]\n",
            " [ 1.29353073 -0.74586075  1.18133462  1.14781109 -0.27408406]\n",
            " [-0.11641085  0.00612881 -0.98863151  1.8065881   0.22385202]\n",
            " [ 0.57119461  0.538934   -0.86930624 -1.50535665 -2.45548554]\n",
            " [-1.24688279 -0.28177836  1.19660195 -0.88771766  1.53179332]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# minmax-scaler aus sklearn erstellen\n",
        "scaler_minmax = MinMaxScaler()\n",
        "# scaler fitten (min-max der features berechnen) und transformen (das wirkliche anpassen jedes wertes an das gefittete)\n",
        "x_minmax_sklearn = scaler_minmax.fit_transform(x)\n",
        "\n",
        "print(\"Sklearn MinMax-Scaler:\\n\", x_minmax_sklearn)\n",
        "print(\"Differenz: \", np.sum(x_minmax - x_minmax_sklearn))\n",
        "\n",
        "print(\"Verglichen mit der eigenen Implementierung, sieht man, dass unsere Implementierung ebenfalls richtig sein sollte. Differenz ist sehr gering.\")"
      ],
      "metadata": {
        "id": "6mSD-pB3L07-",
        "outputId": "4d2c4100-9da9-48da-c306-4849e3d4cdc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sklearn MinMax-Scaler:\n",
            " [[0.40476159 0.         0.17716244 0.27016344 0.77448957]\n",
            " [0.84425891 0.79578881 0.         0.70276872 0.3206469 ]\n",
            " [0.59055143 0.0340245  0.82246469 0.         0.38462419]\n",
            " [0.22575837 0.74033427 1.         0.61025864 0.77448137]\n",
            " [0.9039843  0.27394465 0.42086525 0.28601121 1.        ]\n",
            " [0.28053095 0.78636198 0.61146714 0.68207098 0.99822039]\n",
            " [0.69682372 0.27853834 0.1201718  0.72880628 0.28458586]\n",
            " [0.         0.19147452 0.55615255 0.6246069  0.87092676]\n",
            " [1.         1.         0.62533191 0.70996065 0.        ]\n",
            " [0.59888451 0.68412728 0.36641176 1.         0.41290151]]\n",
            "Differenz:  2.0816681711721685e-16\n",
            "Verglichen mit der eigenen Implementierung, sieht man, dass unsere Implementierung ebenfalls richtig sein sollte. Differenz ist sehr gering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# standard-scaler aus sklearn erstellen\n",
        "scaler_standard = StandardScaler()\n",
        "# scaler fitten (mean und std der features berechnen) und transformen (das wirkliche anpassen jedes wertes an das gefittete)\n",
        "x_standard_sklearn = scaler_standard.fit_transform(x)\n",
        "\n",
        "print(\"Sklearn Standard-Scaler:\\n\", x_standard_sklearn)\n",
        "print(\"Differenz: \", np.sum(x_standard - x_standard_sklearn))\n",
        "\n",
        "print(\"Verglichen mit der eigenen Implementierung, sieht man, dass unsere Implementierung ebenfalls richtig sein sollte. Differenz ist 0.\")"
      ],
      "metadata": {
        "id": "Q853zJUxL8xw",
        "outputId": "5f001058-e621-466f-bb59-498f3322f5b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sklearn Standard-Scaler:\n",
            " [[ 0.11799284 -1.12805277  0.02974996  0.13646306  0.63115847]\n",
            " [-0.92152661 -0.95868573 -0.35009711 -0.69347962  0.54993941]\n",
            " [ 1.15160204  2.55352086 -0.53507049 -0.74207601  0.11676113]\n",
            " [ 1.34497229 -0.18915711  1.52785078  0.39925133 -0.76008397]\n",
            " [-0.98295316  0.49297021  0.39989602  0.91010791 -0.0182514 ]\n",
            " [-1.2115191  -0.28801916 -1.59232798 -0.57159153  0.45440062]\n",
            " [ 1.29353073 -0.74586075  1.18133462  1.14781109 -0.27408406]\n",
            " [-0.11641085  0.00612881 -0.98863151  1.8065881   0.22385202]\n",
            " [ 0.57119461  0.538934   -0.86930624 -1.50535665 -2.45548554]\n",
            " [-1.24688279 -0.28177836  1.19660195 -0.88771766  1.53179332]]\n",
            "Differenz:  0.0\n",
            "Verglichen mit der eigenen Implementierung, sieht man, dass unsere Implementierung ebenfalls richtig sein sollte. Differenz ist 0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8klM070kaja"
      },
      "source": [
        "### Exercise 2 - Softmax\n",
        "\n",
        "**Summary:** In this exercise you will implement the softmax activation using the naive and numerically\\\n",
        "more stable log-sum variation.\n",
        "\n",
        "\n",
        "**Provided Code:** In the cell below there is some sample code that generates sample inputs.\n",
        "\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "1. Implement the softmax function using the naive approach.\n",
        "2. Implement the softmax function using the log-sum trick.\n",
        "3. Compare your two implementations for numerical stability\\\n",
        "(experiment with different values of std) and verify\n",
        "your results using ```tf.nn.softmax```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "cva3EHhikaja"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "mu = 0\n",
        "std = 10\n",
        "xi = mu + std * np.random.randn(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# naiver ansatz\n",
        "def naive_softmax(x):\n",
        "  # (e hoch xi) fuer alle xi berechnen\n",
        "  exp_x = np.exp(x)\n",
        "  # diese wert jeweils durch die summe von allen e hoch xi rechnen\n",
        "  return exp_x / np.sum(exp_x)\n",
        "\n",
        "naive_softmax_values = naive_softmax(xi)\n",
        "\n",
        "print(\"Verteilung: \", naive_softmax_values)\n",
        "print(\"Summe der Verteilung: \", np.sum(naive_softmax_values))\n",
        "print(\"Die Summe der Werte ergibt annaehernd 1 was richtig erscheint.\")"
      ],
      "metadata": {
        "id": "8Gy5u41tM941",
        "outputId": "1d030539-5ab4-499c-9945-9050e286578f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verteilung:  [7.77678456e-12 6.82935783e-09 3.35587511e-06 1.23765881e-04\n",
            " 1.31781013e-13 7.58496347e-10 1.86214126e-10 3.12824743e-11\n",
            " 9.99871636e-01 1.23448872e-06]\n",
            "Summe der Verteilung:  1.0000000000000002\n",
            "Die Summe der Werte ergibt annaehernd 1 was richtig erscheint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_logsum(x):\n",
        "  # max xi als c speichern\n",
        "  c = np.max(x)\n",
        "  # (e hoch (xi - c)) fuer alle xi berechnen\n",
        "  exp_x = np.exp(x - c)\n",
        "  # denumerator (log(d))\n",
        "  ld = c + np.log(np.sum(exp_x))\n",
        "  # e hoch (xi - log(d))\n",
        "  return np.exp(x - ld)\n",
        "\n",
        "logsum_softmax_values = softmax_logsum(xi)\n",
        "\n",
        "print(\"Verteilung Logsum-Trick: \", logsum_softmax_values)\n",
        "print(\"Summe der Verteilung: \", np.sum(naive_softmax_values))\n",
        "print(\"Die Summe der Werte beim LogSum-Trick ergibt annaehernd 1 was richtig erscheint.\")"
      ],
      "metadata": {
        "id": "-sZdwaM0PskF",
        "outputId": "912da66c-049f-40f1-c6a7-a2144a47a8fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verteilung Logsum-Trick:  [7.77678456e-12 6.82935783e-09 3.35587511e-06 1.23765881e-04\n",
            " 1.31781013e-13 7.58496347e-10 1.86214126e-10 3.12824743e-11\n",
            " 9.99871636e-01 1.23448872e-06]\n",
            "Summe der Verteilung:  1.0000000000000002\n",
            "Die Summe der Werte beim LogSum-Trick ergibt annaehernd 1 was richtig erscheint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Naive vs LogSum-Trick: \", np.sum(naive_softmax_values - logsum_softmax_values))"
      ],
      "metadata": {
        "id": "eF2gZZ58RrN7",
        "outputId": "17aa538c-9d97-404b-fc95-2e2eae93bddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive vs LogSum-Trick:  1.3324380944172428e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xi2 = mu + 1000 * np.random.randn(10)\n",
        "\n",
        "ns = naive_softmax(xi2)\n",
        "print(\"Fuer groessere Werte bekommen wir einen Overflow-Warning bei der naiven Variante: \", ns)\n",
        "\n",
        "ls = softmax_logsum(xi2)\n",
        "print(\"Hingegen kann die logsum-variante immernoch Werte berechnen: \", ls)"
      ],
      "metadata": {
        "id": "RNTdXZ57UVYz",
        "outputId": "06b08c5d-a296-491b-854f-841fcdc17481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fuer groessere Werte bekommen wir einen Overflow-Warning bei der naiven Variante:  [ 0.  0.  0.  0. nan  0.  0.  0.  0.  0.]\n",
            "Hingegen kann die logsum-variante immernoch Werte berechnen:  [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            " 1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            " 6.99411192e-245 0.00000000e+000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-e99606a99de6>:4: RuntimeWarning: overflow encountered in exp\n",
            "  exp_x = np.exp(x)\n",
            "<ipython-input-79-e99606a99de6>:6: RuntimeWarning: invalid value encountered in divide\n",
            "  return exp_x / np.sum(exp_x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_softmax = tf.nn.softmax(xi).numpy()\n",
        "\n",
        "print(\"Differenz Naive-TF: \", np.sum(naive_softmax_values - tf_softmax))\n",
        "print(\"Differenz Logsum-Tf: \", np.sum(logsum_softmax_values - tf_softmax))\n",
        "\n",
        "print(\"Bringt uns sehr kleine Zahlen. Unsere Implementierungen scheinen in dem Fall nicht all zu falsch zu sein.\")"
      ],
      "metadata": {
        "id": "HXauULiMPI0r",
        "outputId": "45c6e921-1945-41b2-ce34-5bb6f6d5282c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-daa17301c5a6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Differenz Naive-TF: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_softmax_values\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Differenz Logsum-Tf: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogsum_softmax_values\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJtjjh6Ekajb"
      },
      "source": [
        "### Exercise 3 - Chess Endgames\n",
        "\n",
        "**Summary:** In this exercise your task is to predict the optimal depth-of-win for white in   \n",
        "chess-endgames. In particular, we will focus on **king-rook** vs. **king** endgames. The   \n",
        "possible outcomes are either a **draw** or a **number of moves** for white to win (0 to 16).\n",
        "\n",
        "\n",
        "**Provided Code:** The code below loads the original (*unprepared*) raw dataset.   \n",
        "You will have to prepare it accordingly to be used with neural nets.\n",
        "\n",
        "The structure of each row in the dataset is:\n",
        "1. White King column (a-h)\n",
        "2. White King row (1-8)\n",
        "3. White Rook column (a-h)\n",
        "4. White Rook row (1-8)\n",
        "5. Black King column (a-h)\n",
        "6. Black King row (1-8)\n",
        "7. Optimal depth-of-win for White in 0 to 16 moves or a draw\n",
        "\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "1. Train a neural net to predict the depth-of-win (or draw) given a board position\n",
        "    * You will have to prepare your data accordingly to make it compatible   \n",
        "    with neural nets. Think about input and output encodings, normalization or standardization.\n",
        "    * Decide how you will model this problem as either regression or classification task.\n",
        "    * Build a fully connected neural net with appropriate configuration and loss and train it.\n",
        "    * Use appropriate cross-validation for training and validation (it is enough to use two datasets)\n",
        "2. Explain in writing:\n",
        "    * How and why did you prepared the data?\n",
        "    * How did you model the problem task?\n",
        "    * What is your neural network architecture/configuration/loss?\n",
        "    * Plot your loss while training.\n",
        "    * Interpret and explain your results.\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "YSy3aOl4kajb",
        "outputId": "5137e92c-01aa-4834-9b91-50295cee1737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-09 20:23:45--  https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/chess_endgames.pickle\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/shegenbart/Jupyter-Exercises/main/data/chess_endgames.pickle [following]\n",
            "--2025-02-09 20:23:46--  https://raw.githubusercontent.com/shegenbart/Jupyter-Exercises/main/data/chess_endgames.pickle\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6284700 (6.0M) [application/octet-stream]\n",
            "Saving to: ‘../data/chess_endgames.pickle.2’\n",
            "\n",
            "chess_endgames.pick 100%[===================>]   5.99M  18.1MB/s    in 0.3s    \n",
            "\n",
            "2025-02-09 20:23:47 (18.1 MB/s) - ‘../data/chess_endgames.pickle.2’ saved [6284700/6284700]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/chess_endgames.pickle -P ../data\n",
        "import pickle\n",
        "with open('../data/chess_endgames.pickle', 'rb') as fd:\n",
        "    chess_endgames = pickle.load(fd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "h9G8NNclkajb",
        "outputId": "3e9a6737-912d-4eab-b669-88e94a7ee5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'a'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-07220111fd36>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Normalize board positions (scale between 0 and 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Convert target variable to one-hot encoding for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'a'"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert dataset to NumPy array\n",
        "data = np.array(chess_endgames)\n",
        "\n",
        "# Extract features (piece positions) and target variable (depth-of-win)\n",
        "X_raw = data[:, :-1]  # First 6 columns (positions)\n",
        "y_raw = data[:, -1]   # Last column (depth-of-win or draw)\n",
        "\n",
        "# Convert chess board columns (a-h) to numerical (1-8) if necessary\n",
        "# Assuming they are already numerical; otherwise, apply a mapping.\n",
        "\n",
        "# Normalize board positions (scale between 0 and 1)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X_raw)\n",
        "\n",
        "# Convert target variable to one-hot encoding for classification\n",
        "y_raw = y_raw.reshape(-1, 1)  # Reshape for encoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y_raw)\n",
        "\n",
        "# Split dataset into training (80%) and validation (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Neural Network Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(6,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(18, activation='softmax')  # 18 possible outcomes\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                    epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "# Plot Training Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}